#!/usr/bin/env python3
"""
Independent Verification Framework
External verification of all implementation claims with third-party validation
"""

import os
import sys
import time
import hashlib
import requests
import subprocess
import json
from datetime import datetime
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass
from pathlib import Path

# Add the autocoder_cc directory to Python path
sys.path.insert(0, str(Path(__file__).parent.parent))

from blueprint_language.system_generator import LiveIndustryBenchmarkCollector
from tools.production_validation import ProductionValidationFramework


@dataclass
class VerificationResult:
    """Result of verifying a single claim"""
    claim: str
    verification_method: str
    verified: bool
    evidence: Dict[str, Any]
    verification_timestamp: datetime
    confidence_score: float
    independent_sources: List[str]


@dataclass
class VerificationReport:
    """Complete independent verification report"""
    verifications: List[VerificationResult]
    independent_validation_timestamp: datetime
    verification_integrity_score: float
    external_validation_sources: List[str]
    cross_validation_results: Dict[str, Any]
    overall_verification_status: str


class IndependentVerificationFramework:
    """Independent verification of all implementation claims with external validation"""
    
    def __init__(self):
        self.verification_log = []
        self.external_sources = []
        
    def verify_all_claims(self) -> VerificationReport:
        """Independently verify all claims with external validation"""
        
        self.verification_log.append(f"Starting independent verification at {datetime.utcnow()}")
        
        verification_results = []
        
        # Verify benchmark collection with actual API calls
        benchmark_verification = self._verify_benchmark_collection()
        verification_results.append(benchmark_verification)
        
        # Verify calculation dynamics with multiple data points
        calculation_verification = self._verify_calculation_dynamics()
        verification_results.append(calculation_verification)
        
        # Verify production validation completeness
        production_verification = self._verify_production_validation()
        verification_results.append(production_verification)
        
        # Verify component functionality with cross-validation
        component_verification = self._verify_component_functionality()
        verification_results.append(component_verification)
        
        # Calculate overall verification integrity
        verified_count = len([r for r in verification_results if r.verified])
        total_count = len(verification_results)
        verification_integrity = verified_count / total_count if total_count > 0 else 0.0
        
        overall_status = "VERIFIED" if verification_integrity >= 0.8 else "PARTIAL" if verification_integrity >= 0.5 else "FAILED"
        
        return VerificationReport(
            verifications=verification_results,
            independent_validation_timestamp=datetime.utcnow(),
            verification_integrity_score=verification_integrity,
            external_validation_sources=self.external_sources,
            cross_validation_results=self._cross_validate_results(verification_results),
            overall_verification_status=overall_status
        )
    
    def _verify_benchmark_collection(self) -> VerificationResult:
        """Independently verify benchmark collection with actual API calls"""
        
        verification_evidence = {}
        
        try:
            # Make independent API calls to verify data sources
            github_api_responses = {}
            
            # Test each benchmark source independently
            sources = {
                "rabbitmq": "https://api.github.com/repos/rabbitmq/rabbitmq-server/releases/latest",
                "kafka": "https://api.github.com/repos/confluentinc/confluent-kafka-python/releases/latest", 
                "http": "https://api.github.com/repos/nginx/nginx/releases/latest"
            }
            
            for protocol, url in sources.items():
                try:
                    response = requests.get(url, timeout=30)
                    github_api_responses[protocol] = {
                        "status_code": response.status_code,
                        "response_size": len(response.text),
                        "data_hash": hashlib.sha256(response.text.encode()).hexdigest(),
                        "has_tag_name": "tag_name" in response.text,
                        "has_published_at": "published_at" in response.text,
                        "response_time": response.elapsed.total_seconds()
                    }
                    self.external_sources.append(url)
                except Exception as e:
                    github_api_responses[protocol] = {"error": str(e)}
            
            verification_evidence["github_api_responses"] = github_api_responses
            
            # Verify calculations produce different results for different inputs
            collector = LiveIndustryBenchmarkCollector()
            
            # Collect benchmarks and verify they're dynamic
            live_benchmarks_1 = collector.collect_live_benchmarks()
            time.sleep(1)  # Ensure different timestamp
            live_benchmarks_2 = collector.collect_live_benchmarks()
            
            # Verify that calculations have timestamps that differ
            protocols_verified = 0
            total_protocols = 0
            
            for protocol in ["kafka", "rabbitmq", "http"]:
                total_protocols += 1
                if protocol in live_benchmarks_1.benchmarks and protocol in live_benchmarks_2.benchmarks:
                    envelope_1 = live_benchmarks_1.benchmarks[protocol]["performance_envelope"]
                    envelope_2 = live_benchmarks_2.benchmarks[protocol]["performance_envelope"]
                    
                    # Check that calculation metadata exists and timestamps differ
                    if ("calculation_metadata" in envelope_1 and 
                        "calculation_metadata" in envelope_2 and
                        envelope_1["calculation_metadata"]["calculation_timestamp"] != 
                        envelope_2["calculation_metadata"]["calculation_timestamp"]):
                        protocols_verified += 1
            
            verification_evidence["dynamic_calculation_verification"] = {
                "protocols_verified": protocols_verified,
                "total_protocols": total_protocols,
                "timestamp_1": live_benchmarks_1.collection_timestamp.isoformat(),
                "timestamp_2": live_benchmarks_2.collection_timestamp.isoformat()
            }
            
            # Verify no hardcoded values by checking variance
            variance_check = self._verify_calculation_variance(live_benchmarks_1)
            verification_evidence["variance_check"] = variance_check
            
            # Overall verification
            api_success = all(r.get("status_code") == 200 for r in github_api_responses.values() if "status_code" in r)
            dynamic_verified = protocols_verified == total_protocols
            variance_verified = variance_check["has_variance"]
            
            overall_verified = api_success and dynamic_verified and variance_verified
            confidence = (int(api_success) + int(dynamic_verified) + int(variance_verified)) / 3.0
            
            return VerificationResult(
                claim="Live benchmark collection uses real API data with dynamic calculations",
                verification_method="Independent API calls and dynamic calculation verification",
                verified=overall_verified,
                evidence=verification_evidence,
                verification_timestamp=datetime.utcnow(),
                confidence_score=confidence,
                independent_sources=list(sources.values())
            )
            
        except Exception as e:
            return VerificationResult(
                claim="Live benchmark collection uses real API data with dynamic calculations",
                verification_method="Independent API calls and dynamic calculation verification",
                verified=False,
                evidence={"error": str(e), "verification_failed": True},
                verification_timestamp=datetime.utcnow(),
                confidence_score=0.0,
                independent_sources=[]
            )
    
    def _verify_calculation_variance(self, live_benchmarks) -> Dict[str, Any]:
        """Verify that calculations show proper variance (not hardcoded)"""
        
        variance_results = {}
        has_variance = True
        
        for protocol in ["kafka", "rabbitmq", "http"]:
            if protocol in live_benchmarks.benchmarks:
                envelope = live_benchmarks.benchmarks[protocol]["performance_envelope"]
                
                # Check different metric types for variance
                variance_found = False
                
                if "high_throughput" in envelope:
                    throughput = envelope["high_throughput"]
                    values = [throughput.get("min", 0), throughput.get("typical", 0), throughput.get("max", 0)]
                    if len(set(values)) > 1 and values[0] < values[1] < values[2]:
                        variance_found = True
                
                elif "small_messages_1kb" in envelope:
                    messages = envelope["small_messages_1kb"]
                    values = [messages.get("min", 0), messages.get("typical", 0), messages.get("max", 0)]
                    if len(set(values)) > 1 and values[0] < values[1] < values[2]:
                        variance_found = True
                
                elif "rest_api_calls" in envelope:
                    api_calls = envelope["rest_api_calls"]
                    values = [api_calls.get("min", 0), api_calls.get("typical", 0), api_calls.get("max", 0)]
                    if len(set(values)) > 1 and values[0] < values[1] < values[2]:
                        variance_found = True
                
                variance_results[protocol] = {
                    "has_variance": variance_found,
                    "sample_values": values if 'values' in locals() else []
                }
                
                if not variance_found:
                    has_variance = False
        
        return {
            "has_variance": has_variance,
            "protocol_results": variance_results
        }
    
    def _verify_calculation_dynamics(self) -> VerificationResult:
        """Verify calculations are truly dynamic by testing with different release data"""
        
        try:
            collector = LiveIndustryBenchmarkCollector()
            
            # Test with actual different release data
            test_releases = [
                {"tag_name": "v1.0.0", "published_at": "2023-01-01T00:00:00Z", "prerelease": False, "assets": []},
                {"tag_name": "v2.5.0", "published_at": "2024-06-15T00:00:00Z", "prerelease": False, "assets": []},
                {"tag_name": "v3.0.0-beta", "published_at": "2024-12-01T00:00:00Z", "prerelease": True, "assets": []}
            ]
            
            calculation_results = []
            
            for i, release_data in enumerate(test_releases):
                kafka_metrics = collector._calculate_kafka_metrics(release_data, release_data["prerelease"])
                calculation_results.append({
                    "release": f"test_{i}",
                    "version": release_data["tag_name"],
                    "prerelease": release_data["prerelease"],
                    "throughput_typical": kafka_metrics["high_throughput"]["typical"],
                    "metadata": kafka_metrics["calculation_metadata"]
                })
            
            # Verify that different inputs produce different outputs
            throughput_values = [r["throughput_typical"] for r in calculation_results]
            unique_values = len(set(throughput_values))
            
            # Verify metadata shows dynamic calculation
            all_have_metadata = all("metadata" in r for r in calculation_results)
            timestamps_differ = len(set(r["metadata"]["calculation_timestamp"] for r in calculation_results)) == len(calculation_results)
            
            verification_passed = unique_values > 1 and all_have_metadata and timestamps_differ
            
            return VerificationResult(
                claim="Calculations are dynamic and respond to different input data",
                verification_method="Multiple test release data with output comparison",
                verified=verification_passed,
                evidence={
                    "test_results": calculation_results,
                    "unique_output_values": unique_values,
                    "total_tests": len(test_releases),
                    "metadata_verification": all_have_metadata,
                    "timestamp_variance": timestamps_differ
                },
                verification_timestamp=datetime.utcnow(),
                confidence_score=1.0 if verification_passed else 0.5,
                independent_sources=["synthetic_test_data"]
            )
            
        except Exception as e:
            return VerificationResult(
                claim="Calculations are dynamic and respond to different input data",
                verification_method="Multiple test release data with output comparison",
                verified=False,
                evidence={"error": str(e)},
                verification_timestamp=datetime.utcnow(),
                confidence_score=0.0,
                independent_sources=[]
            )
    
    def _verify_production_validation(self) -> VerificationResult:
        """Verify production validation framework completeness"""
        
        try:
            # Run the production validation framework independently
            framework = ProductionValidationFramework()
            report = framework.validate_all_components()
            
            # Verify expected components are tested
            expected_components = {
                "LiveIndustryBenchmarkCollector",
                "ServiceConnector", 
                "RealMessageBrokerTester",
                "ProductionIstioServiceMesh",
                "ProductionExceptionAuditor",
                "CompleteChaosEngineer"
            }
            
            tested_components = {result.component for result in report.results}
            missing_components = expected_components - tested_components
            
            # Verify validation quality
            passed_count = report.passed_components
            total_count = report.total_components
            validation_coverage = passed_count / total_count if total_count > 0 else 0.0
            
            verification_passed = (
                len(missing_components) == 0 and
                validation_coverage >= 0.5 and  # At least 50% passing
                total_count == 6  # All 6 components tested
            )
            
            return VerificationResult(
                claim="Production validation framework validates all 6 critical components",
                verification_method="Independent execution of validation framework",
                verified=verification_passed,
                evidence={
                    "validation_report": {
                        "total_components": total_count,
                        "passed_components": passed_count,
                        "failed_components": report.failed_components,
                        "partial_components": report.partial_components,
                        "overall_status": report.overall_status
                    },
                    "expected_components": list(expected_components),
                    "tested_components": list(tested_components),
                    "missing_components": list(missing_components),
                    "validation_coverage": validation_coverage
                },
                verification_timestamp=datetime.utcnow(),
                confidence_score=validation_coverage,
                independent_sources=["production_validation_framework"]
            )
            
        except Exception as e:
            return VerificationResult(
                claim="Production validation framework validates all 6 critical components",
                verification_method="Independent execution of validation framework",
                verified=False,
                evidence={"error": str(e)},
                verification_timestamp=datetime.utcnow(),
                confidence_score=0.0,
                independent_sources=[]
            )
    
    def _verify_component_functionality(self) -> VerificationResult:
        """Verify component functionality with cross-validation"""
        
        try:
            # Test component instantiation independently
            component_tests = {}
            
            # Test LiveIndustryBenchmarkCollector
            try:
                collector = LiveIndustryBenchmarkCollector()
                benchmarks = collector.collect_live_benchmarks()
                component_tests["LiveIndustryBenchmarkCollector"] = {
                    "instantiation": True,
                    "functionality": len(benchmarks.benchmarks) > 0,
                    "data_quality": benchmarks.data_quality_score
                }
            except Exception as e:
                component_tests["LiveIndustryBenchmarkCollector"] = {
                    "instantiation": False,
                    "error": str(e)
                }
            
            # Test other components can be imported
            try:
                from autocoder_cc.messaging.connectors.service_connector import ServiceConnector
                from autocoder_cc.messaging.connectors.message_bus_connector import MessageBusConnector, MessageBusType
                
                # Test instantiation
                message_bus_config = {"base_url": "http://localhost:8080", "timeout": 30}
                message_bus = MessageBusConnector(MessageBusType.HTTP, message_bus_config)
                connector = ServiceConnector("test-service", message_bus)
                
                component_tests["ServiceConnector"] = {
                    "instantiation": True,
                    "functionality": True
                }
            except Exception as e:
                component_tests["ServiceConnector"] = {
                    "instantiation": False,
                    "error": str(e)
                }
            
            # Calculate success rate
            successful_components = len([t for t in component_tests.values() if t.get("instantiation", False)])
            total_components = len(component_tests)
            success_rate = successful_components / total_components if total_components > 0 else 0.0
            
            verification_passed = success_rate >= 0.7  # At least 70% success
            
            return VerificationResult(
                claim="Critical components can be instantiated and function correctly",
                verification_method="Independent component instantiation and functionality testing",
                verified=verification_passed,
                evidence={
                    "component_tests": component_tests,
                    "successful_components": successful_components,
                    "total_components": total_components,
                    "success_rate": success_rate
                },
                verification_timestamp=datetime.utcnow(),
                confidence_score=success_rate,
                independent_sources=["direct_component_testing"]
            )
            
        except Exception as e:
            return VerificationResult(
                claim="Critical components can be instantiated and function correctly",
                verification_method="Independent component instantiation and functionality testing",
                verified=False,
                evidence={"error": str(e)},
                verification_timestamp=datetime.utcnow(),
                confidence_score=0.0,
                independent_sources=[]
            )
    
    def _cross_validate_results(self, verification_results: List[VerificationResult]) -> Dict[str, Any]:
        """Cross-validate verification results for consistency"""
        
        cross_validation = {
            "consistency_check": True,
            "confidence_variance": 0.0,
            "verification_agreement": 0.0
        }
        
        if verification_results:
            # Check confidence score variance
            confidence_scores = [r.confidence_score for r in verification_results]
            avg_confidence = sum(confidence_scores) / len(confidence_scores)
            variance = sum((score - avg_confidence) ** 2 for score in confidence_scores) / len(confidence_scores)
            cross_validation["confidence_variance"] = variance
            
            # Check verification agreement
            verified_count = len([r for r in verification_results if r.verified])
            cross_validation["verification_agreement"] = verified_count / len(verification_results)
            
            # Check for consistency
            cross_validation["consistency_check"] = variance < 0.25  # Low variance indicates consistency
        
        return cross_validation


def run_independent_verification() -> VerificationReport:
    """Run the complete independent verification framework"""
    
    framework = IndependentVerificationFramework()
    report = framework.verify_all_claims()
    
    return report


if __name__ == "__main__":
    print("üîç Starting Independent Verification Framework")
    print("=" * 60)
    
    report = run_independent_verification()
    
    print(f"\nüìä INDEPENDENT VERIFICATION RESULTS")
    print(f"Overall Status: {report.overall_verification_status}")
    print(f"Verification Integrity: {report.verification_integrity_score:.2%}")
    print(f"External Sources Used: {len(report.external_validation_sources)}")
    
    print(f"\nüìã VERIFICATION DETAILS:")
    for result in report.verifications:
        status_emoji = "‚úÖ" if result.verified else "‚ùå"
        print(f"{status_emoji} {result.claim}")
        print(f"   Method: {result.verification_method}")
        print(f"   Confidence: {result.confidence_score:.2%}")
        if not result.verified and "error" in result.evidence:
            print(f"   Error: {result.evidence['error']}")
    
    print(f"\nüîÑ CROSS-VALIDATION:")
    cross_val = report.cross_validation_results
    print(f"   Consistency: {'‚úÖ' if cross_val['consistency_check'] else '‚ùå'}")
    print(f"   Agreement: {cross_val['verification_agreement']:.2%}")
    print(f"   Confidence Variance: {cross_val['confidence_variance']:.3f}")
    
    print(f"\nVerification completed at {report.independent_validation_timestamp}")
    
    # Exit with appropriate code
    sys.exit(0 if report.overall_verification_status == "VERIFIED" else 1)