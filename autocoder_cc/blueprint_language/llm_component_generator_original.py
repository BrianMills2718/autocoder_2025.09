#!/usr/bin/env python3
"""
LLM Component Generator - Refactored and Modularized

This is the main orchestrator for LLM-based component generation.
The previous 2,119-line monolithic file has been broken down into focused modules:

- llm_generation/o3_specialized_prompts.py: O3-specific prompt engineering  
- llm_generation/response_validator.py: LLM response validation
- llm_generation/prompt_engine.py: Core prompt generation  
- llm_generation/retry_orchestrator.py: Retry logic with improved prompts
- llm_generation/context_builder.py: Context-aware prompt building

This refactored version focuses on orchestration and high-level logic.
"""

import os
import json
import time
from typing import Dict, Any, Optional
from .llm_generation.component_wrapper import wrap_component_with_boilerplate
from dotenv import load_dotenv

# Multi-provider LLM abstraction imports
from autocoder_cc.llm_providers.multi_provider_manager import MultiProviderManager
from autocoder_cc.llm_providers.openai_provider import OpenAIProvider
from autocoder_cc.llm_providers.anthropic_provider import AnthropicProvider
from autocoder_cc.llm_providers.gemini_provider import GeminiProvider
from autocoder_cc.llm_providers.base_provider import LLMRequest

from autocoder_cc.core.config import settings
from autocoder_cc.error_handling import ConsistentErrorHandler
from autocoder_cc.observability.structured_logging import get_logger
from autocoder_cc.observability.metrics import get_metrics_collector

# Extracted modules
from .llm_generation.o3_specialized_prompts import O3PromptEngine
from .llm_generation.response_validator import ResponseValidator, ComponentGenerationError
from .llm_generation.prompt_engine import PromptEngine
from .llm_generation.retry_orchestrator import RetryOrchestrator, ErrorType
from .llm_generation.context_builder import ContextBuilder

load_dotenv()


class LLMComponentGenerator:
    """
    Refactored LLM Component Generator with modular architecture
    
    This class now orchestrates the generation process using focused modules:
    - O3PromptEngine: Handles O3-specific prompt engineering
    - ResponseValidator: Validates generated code for quality and correctness
    - PromptEngine: Core prompt generation with embedded base classes
    - RetryOrchestrator: Manages retry logic with progressive prompt improvement
    - ContextBuilder: Builds context-aware prompts for system integration
    
    Key Principles:
    - ALL component logic MUST be generated by LLM
    - NO hardcoded templates or placeholders
    - NO fallback modes or mock implementations
    - Fail hard if LLM is unavailable
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        """Initialize modular LLM system with extracted components"""
        
        # Initialize structured logger
        self.logger = get_logger(__name__, component="llm_component_generator")
        
        # Initialize metrics collector
        self.metrics = get_metrics_collector("llm_component_generator")
        
        # Use provided config or default from settings
        if config is None:
            config = {
                "llm_providers": {
                    "primary_provider": "openai",  # Use OpenAI for o3 model support
                    "fallback_providers": ["gemini", "anthropic"],
                    "max_retries": 3
                },
                "openai_model": os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")  # Use model from environment
            }
        
        # Initialize multi-provider system
        provider_config = config.get("llm_providers", {
            "primary_provider": "openai",  # Use OpenAI for o3 model support
            "fallback_providers": ["gemini", "anthropic"],
            "max_retries": 3
        })
        
        self.multi_provider = MultiProviderManager(provider_config)
        
        # Register providers based on configuration
        primary_provider = provider_config["primary_provider"]
        fallback_providers = provider_config.get("fallback_providers", [])
        all_providers = [primary_provider] + fallback_providers
        
        if "openai" in all_providers:
            openai_config = {
                "api_key": config.get("openai_api_key") or os.getenv("OPENAI_API_KEY"),
                "default_model": config.get("openai_model", os.getenv("OPENAI_MODEL", "gpt-3.5-turbo"))  # Use model from environment
            }
            if openai_config["api_key"]:
                openai_provider = OpenAIProvider(openai_config)
                self.multi_provider.registry.register_provider(openai_provider)
                self.logger.info("Registered OpenAI provider")
        
        if "anthropic" in all_providers:
            anthropic_config = {
                "api_key": config.get("anthropic_api_key") or os.getenv("ANTHROPIC_API_KEY"),
                "default_model": config.get("anthropic_model", "claude-3-5-sonnet-20241022")
            }
            if anthropic_config["api_key"]:
                anthropic_provider = AnthropicProvider(anthropic_config)
                self.multi_provider.registry.register_provider(anthropic_provider)
                self.logger.info("Registered Anthropic provider")
        
        if "gemini" in all_providers:
            gemini_config = {
                "api_key": config.get("gemini_api_key") or os.getenv("GEMINI_API_KEY"),
                "default_model": config.get("gemini_model", "gemini-2.5-flash")
            }
            if gemini_config["api_key"]:
                gemini_provider = GeminiProvider(gemini_config)
                self.multi_provider.registry.register_provider(gemini_provider)
                self.logger.info("Registered Gemini provider")
        
        # Verify at least one provider is registered
        if not self.multi_provider.registry.list_providers():
            raise ComponentGenerationError(
                "No LLM providers available. Set GEMINI_API_KEY, OPENAI_API_KEY and/or ANTHROPIC_API_KEY. "
                "Multi-provider system requires at least one configured provider."
            )
        
        self.logger.info("Multi-provider LLM system initialized", {
            "providers": self.multi_provider.registry.list_providers(),
            "primary_provider": provider_config["primary_provider"]
        })
        
        # Initialize extracted modules
        self.o3_prompt_engine = O3PromptEngine()
        self.response_validator = ResponseValidator()
        self.prompt_engine = PromptEngine()
        self.retry_orchestrator = RetryOrchestrator(
            max_retries=settings.MAX_RETRIES,
            base_delay=settings.RETRY_DELAY
        )
        self.context_builder = ContextBuilder()
        
        # Error handling configuration
        self.circuit_breaker_failures = 0
        self.circuit_breaker_threshold = 5
        self.circuit_breaker_timeout = 60  # seconds
        
        # Setup consistent error handler
        self.error_handler = ConsistentErrorHandler("LLMComponentGenerator")
        
        self.logger.info("Modular LLM Component Generator initialized")
    
    async def generate_component_implementation_enhanced(
        self, 
        component_type: str,
        component_name: str,
        component_description: str,
        component_config: Dict[str, Any],
        class_name: str,
        system_context: Optional[Dict[str, Any]] = None,
        blueprint: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        Generate component implementation with enhanced context-aware prompting
        
        Args:
            component_type: Type of component (Source, Transformer, etc.)
            component_name: Name of the component
            component_description: Description of component functionality
            component_config: Component configuration
            class_name: Generated class name
            system_context: System-wide context for enhanced generation
            blueprint: System blueprint containing component connections and data flow
            
        Returns:
            Complete Python implementation with system-aware business logic
            
        Raises:
            ComponentGenerationError: If generation fails
        """
        
        # Build enhanced context-aware prompt
        system_prompt = self.prompt_engine.get_system_prompt(
            component_type, 
            self.o3_prompt_engine.get_reasoning_prefix()
        )
        
        # Use enhanced prompting if system context is available
        if system_context:
            user_prompt = self.context_builder.build_context_aware_prompt(
                component_type, component_name, component_description, 
                component_config, class_name, system_context,
                self.prompt_engine.build_component_prompt, blueprint
            )
        else:
            # Fallback to original prompting for backward compatibility
            user_prompt = self.prompt_engine.build_component_prompt(
                component_type, component_name, component_description, 
                component_config, class_name, blueprint
            )
        
        return await self._generate_with_retry_logic(
            system_prompt, user_prompt, component_type, component_name, class_name
        )

    async def generate_component_implementation(
        self, 
        component_type: str,
        component_name: str,
        component_description: str,
        component_config: Dict[str, Any],
        class_name: str,
        blueprint: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        Generate complete component implementation using LLM
        
        Args:
            component_type: Type of component (Source, Transformer, etc.)
            component_name: Name of the component
            component_description: Description of component functionality
            component_config: Component configuration
            class_name: Generated class name
            blueprint: System blueprint containing component connections and data flow
            
        Returns:
            Complete Python implementation with real business logic
            
        Raises:
            ComponentGenerationError: If generation fails
        """
        
        # Build detailed prompt based on component type
        system_prompt = self.prompt_engine.get_system_prompt(
            component_type, 
            self.o3_prompt_engine.get_reasoning_prefix()
        )
        user_prompt = self.prompt_engine.build_component_prompt(
            component_type, component_name, component_description, 
            component_config, class_name, blueprint
        )
        
        try:
            # Use robust LLM call with retry and validation
            generated_code = await self._generate_with_retry_logic(
                system_prompt, user_prompt, component_type, component_name, class_name
            )
            
            # Save generated code to a debug file for inspection
            import tempfile
            debug_file = os.path.join(tempfile.gettempdir(), f"debug_{component_name}.py")
            with open(debug_file, 'w', encoding='utf-8') as f:
                f.write(generated_code)
            print(f"Debug: Saved generated code to {debug_file}")
            
            return generated_code
            
        except Exception as e:
            raise ComponentGenerationError(
                f"Failed to generate component {component_name} via LLM: {e}\n"
                f"Component type: {component_type}\n"
                f"NO FALLBACKS - LLM generation is mandatory for ALL components."
            )
    
    async def _generate_with_retry_logic(
        self, 
        system_prompt: str, 
        user_prompt: str, 
        component_type: str, 
        component_name: str, 
        class_name: str
    ) -> str:
        """Generate with modular retry logic using extracted orchestrator"""
        
        # Check circuit breaker
        if self.circuit_breaker_failures >= self.circuit_breaker_threshold:
            self.logger.error("circuit_breaker_open", {
                "failures": self.circuit_breaker_failures,
                "threshold": self.circuit_breaker_threshold,
                "timeout": self.circuit_breaker_timeout
            })
            raise ComponentGenerationError(
                f"Circuit breaker open: LLM service unavailable "
                f"({self.circuit_breaker_failures} consecutive failures). "
                f"Try again in {self.circuit_breaker_timeout} seconds."
            )
        
        last_exception = None
        validation_feedback = ""
        
        # Log generation start
        generation_id = f"{component_name}_{int(time.time())}"
        self.logger.info("llm_generation_start", {
            "generation_id": generation_id,
            "component_type": component_type,
            "component_name": component_name,
            "class_name": class_name,
            "provider": "multi-provider",
            "model": "dynamic"
        })
        
        max_attempts = self.retry_orchestrator.retry_strategy.max_retries + 1
        
        for attempt in range(max_attempts):
            try:
                print(f"LLM call attempt {attempt + 1}/{max_attempts}")
                
                # Build adaptive prompt with validation feedback
                if attempt > 0 and validation_feedback:
                    adapted_user_prompt = self.retry_orchestrator.build_adaptive_prompt(
                        user_prompt, validation_feedback, attempt, 
                        self.o3_prompt_engine.get_reasoning_prefix()
                    )
                else:
                    adapted_user_prompt = user_prompt
                
                # Log attempt
                attempt_start = time.time()
                self.logger.info("llm_generation_attempt", {
                    "generation_id": generation_id,
                    "attempt": attempt + 1,
                    "max_attempts": max_attempts,
                    "has_validation_feedback": bool(validation_feedback),
                    "prompt_length": len(system_prompt) + len(adapted_user_prompt)
                })
                
                # Create LLM request with maximum token limit for Gemini
                request = LLMRequest(
                    system_prompt=system_prompt,
                    user_prompt=adapted_user_prompt,
                    max_tokens=8192,  # Use Gemini's full 8K output limit
                    temperature=0.3
                )
                
                # Make the multi-provider API call
                response = await self.multi_provider.generate(request)
                generated_code = response.content
                
                # Post-process the generated code
                generated_code = self._post_process_generated_code(generated_code, component_name)
                
                # Check if LLM followed instructions to generate only the class
                if "import " in generated_code or "from " in generated_code or "class ComposedComponent" in generated_code:
                    # LLM generated boilerplate despite instructions
                    print(f"Warning: LLM generated boilerplate. Extracting component class...")
                    # Try to extract just the component class
                    lines = generated_code.split('\n')
                    class_start = None
                    for i, line in enumerate(lines):
                        if line.strip().startswith(f"class Generated{component_type}_"):
                            class_start = i
                            break
                    
                    if class_start is not None:
                        # Extract from the component class onwards
                        generated_code = '\n'.join(lines[class_start:])
                        print(f"Extracted component class starting at line {class_start}")
                    else:
                        raise ComponentGenerationError(
                            f"LLM did not follow instructions. Generated boilerplate and could not find component class.\n"
                            f"Expected class starting with: class Generated{component_type}_{component_name}"
                        )
                
                # Wrap component with boilerplate
                generated_code = wrap_component_with_boilerplate(
                    component_type=component_type,
                    component_name=component_name,
                    component_code=generated_code
                )
                
                # Validate generated code using extracted validator
                try:
                    self._validate_generated_code(generated_code, component_type, class_name)
                    
                    # Log success
                    self.logger.info("llm_generation_complete", {
                        "generation_id": generation_id,
                        "total_attempts": attempt + 1,
                        "success": True,
                        "provider": response.provider,
                        "model": response.model
                    })
                    
                    # Reset circuit breaker on success
                    self.circuit_breaker_failures = 0
                    print(f"✅ LLM generation successful with {response.provider} after {attempt + 1} attempt(s)")
                    
                    return generated_code
                    
                except ComponentGenerationError as validation_error:
                    # Classify error and prepare for retry
                    error_type = self.retry_orchestrator.classify_error(validation_error)
                    validation_feedback = self.retry_orchestrator.format_validation_feedback(validation_error)
                    print(f"❌ {error_type.value} on attempt {attempt + 1}: {validation_error}")
                    
                    # Log validation failure
                    self.logger.warning("llm_validation_failed", {
                        "generation_id": generation_id,
                        "attempt": attempt + 1,
                        "error_type": error_type.value,
                        "error_message": str(validation_error)
                    })
                    
                    # Check if we should retry
                    if not self.retry_orchestrator.should_retry(error_type, attempt):
                        raise ComponentGenerationError(
                            f"LLM generation failed validation after {attempt + 1} attempts. "
                            f"Final validation error: {validation_error}"
                        )
                    
                    # Wait before next attempt
                    if attempt < max_attempts - 1:
                        delay = self.retry_orchestrator.get_retry_delay(attempt, error_type)
                        print(f"Retrying in {delay:.2f} seconds...")
                        time.sleep(delay)
                    
                    continue
                
            except Exception as e:
                # API or network errors
                error_type = ErrorType.API_ERROR
                last_exception = e
                
                # Log API error
                self.logger.warning("llm_api_error", {
                    "generation_id": generation_id,
                    "attempt": attempt + 1,
                    "error_class": type(e).__name__,
                    "error_message": str(e)
                })
                
                # Increment circuit breaker counter
                self.circuit_breaker_failures += 1
                
                # Check if we should retry
                if not self.retry_orchestrator.should_retry(error_type, attempt):
                    break
                
                # Wait before retrying
                if attempt < max_attempts - 1:
                    delay = self.retry_orchestrator.get_retry_delay(attempt, error_type)
                    print(f"Retrying in {delay:.2f} seconds...")
                    time.sleep(delay)
        
        # All retries failed
        self.logger.error("llm_generation_exhausted", {
            "generation_id": generation_id,
            "total_attempts": max_attempts,
            "last_error": str(last_exception)
        })
        
        raise ComponentGenerationError(
            f"LLM service unavailable after {max_attempts} attempts. "
            f"Last error: {last_exception}. "
            f"NO FALLBACKS AVAILABLE - LLM generation is mandatory."
        )
    
    def _post_process_generated_code(self, generated_code: str, component_name: str) -> str:
        """Post-process generated code to clean up formatting issues"""
        import re
        
        # Handle case where LLM doesn't follow instructions
        print(f"Debug: Raw generated code starts with: {repr(generated_code[:50])}")
        
        # Check if response contains markdown code blocks
        if "```python" in generated_code or "```" in generated_code:
            print("Debug: Detected code block markers in response")
            # Extract code from markdown
            lines = generated_code.split('\n')
            code_lines = []
            in_code = False
            for line in lines:
                if line.strip() == "```python" or (line.strip() == "```" and not in_code):
                    in_code = True
                elif line.strip() == "```" and in_code:
                    in_code = False
                elif in_code:
                    code_lines.append(line)
            if code_lines:
                generated_code = '\n'.join(code_lines)
                print(f"Debug: Extracted {len(code_lines)} lines from code block")
        
        # Debug: Log first few lines of generated code
        print(f"\n--- Generated code preview for {component_name} ---")
        print(f"Total length: {len(generated_code)} chars")
        lines = generated_code.split('\n')[:15]  # Show more lines
        for i, line in enumerate(lines):
            if not line.strip():
                print(f"{i+1}: <empty line>")
            else:
                print(f"{i+1}: {line}")
        lines_count = len(generated_code.split('\n'))
        if lines_count > 15:
            print(f"... ({lines_count - 15} more lines)")
        print("--- End preview ---\n")
        
        return generated_code
    
    def _validate_generated_code(self, code: str, component_type: str, class_name: str) -> None:
        """Validate generated code using extracted validator modules"""
        
        # Use comprehensive functional validation
        if not self.response_validator.validate_functional_implementation(code):
            raise ComponentGenerationError(
                f"Generated code failed functional validation - contains placeholder patterns or insufficient business logic"
            )
        
        # Run component structure validation
        valid, issues = self.response_validator.validate_generated_component(code)
        if not valid:
            raise ComponentGenerationError(
                f"Generated component validation failed: {'; '.join(issues)}"
            )
        
        # Run AST validation
        valid, message = self.response_validator.validate_ast_syntax(code)
        if not valid:
            raise ComponentGenerationError(f"AST validation failed: {message}")
        
        # Run import validation
        valid, missing_imports = self.response_validator.validate_imports(code)
        if not valid:
            raise ComponentGenerationError(
                f"Import validation failed. Missing imports: {', '.join(missing_imports)}"
            )
        
        # Verify it compiles with external Python
        try:
            self.response_validator.validate_with_external_compilation(code)
        except ComponentGenerationError as e:
            raise e
    
    def validate_component_requirements(self, component_spec: Dict[str, Any]) -> bool:
        """Validate component specification before generation"""
        required_fields = ["name", "type", "inputs", "outputs"]
        
        for field in required_fields:
            if field not in component_spec:
                self.logger.error(f"Missing required field: {field}")
                return False
        
        # Validate component type
        valid_types = ["Source", "Sink", "Controller", "Transformer", "Model", "Store", "APIEndpoint"]
        if component_spec["type"] not in valid_types:
            self.logger.error(f"Invalid component type: {component_spec['type']}")
            return False
        
        return True